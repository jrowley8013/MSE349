{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "overall-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fredmd import FredMD\n",
    "import sklearn.pipeline as skpipe\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.preprocessing as skp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "earlier-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-press",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "quiet-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPCAData:\n",
    "    \n",
    "    def __init__(self, Nfactor=None, vintage=None, maxfactor=8, standard_method=2, ic_method=2,\n",
    "                 target=None, train_test_split=[('1960-01-01', '1984-12-01'),('1985-01-01', '2019-12-01')], \n",
    "                 nlags=1, drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]) -> None:\n",
    "        \"\"\"\n",
    "        Create fredmd object\n",
    "        Auguments:\n",
    "        1) Nfactor = None: Number of factors to estimate. If None then estimate number of true factors via information critea\n",
    "        2) vintage = None: Vinatege of data to use in \"year-month\" format (e.g. \"2020-10\"). If None use current vintage\n",
    "        3) maxfactor = 8: Maximimum number of factors to test against information critea. If Nfactor is a number, then this is ignored\n",
    "        4) standard_method = 2: method to standardize data before factors are estimate. 0 = Identity transform, 1 = Demean only, 2 = Demean and stardize to unit variance. Default = 2.\n",
    "        5) ic_method = 2: information critea penalty term. Se\n",
    "        e http://www.columbia.edu/~sn2294/pub/ecta02.pdf page 201, equation 9 for options.\n",
    "        \"\"\"\n",
    "        self.drop_cols = drop_cols\n",
    "        self.train_test_split = [[datetime.strptime(x, '%Y-%m-%d') for x in split] for split in train_test_split]\n",
    "        # Make sure arguments are valid\n",
    "        if standard_method not in [0, 1, 2]:\n",
    "            raise ValueError(f\"standard_method must be in [0, 1, 2], got {standard_method}\")\n",
    "        if ic_method not in [1, 2, 3]:\n",
    "            raise ValueError(f\"ic_method must be in [1, 2, 3], got {ic_method}\")\n",
    "        # Download data\n",
    "        self.rawseries, self.transforms, self.target, self.train_mask, self.test_mask = self.download_data(vintage, \n",
    "                                                                       target if target is not None else \"UNRATE\",\n",
    "                                                                         self.train_test_split)\n",
    "        \n",
    "\n",
    "        self.target_name = target\n",
    "        self.standard_method = standard_method\n",
    "        self.ic_method = ic_method\n",
    "        self.maxfactor = maxfactor\n",
    "        self.Nfactor = Nfactor\n",
    "\n",
    "        self.nlags = nlags\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data(vintage, tgt, train_test_split):\n",
    "        if vintage is None:\n",
    "            url = 'https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/monthly/current.csv'\n",
    "        else:\n",
    "            url = f'https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/monthly/{vintage}.csv'\n",
    "        transforms = pd.read_csv(\n",
    "            url, header=0, nrows=1, index_col=0).transpose()\n",
    "        transforms.index.rename(\"series\", inplace=True)\n",
    "        transforms.columns = ['transform']\n",
    "        transforms = transforms.to_dict()['transform']\n",
    "        data = pd.read_csv(url, names=transforms.keys(), skiprows=2, index_col=0,\n",
    "                           skipfooter=1, engine='python', parse_dates=True, infer_datetime_format=True)\n",
    "        \n",
    "        train_mask = (data.index > train_test_split[0][0]) & (data.index <= train_test_split[0][1])\n",
    "        test_mask = (data.index > train_test_split[1][0]) & (data.index <= train_test_split[1][1])\n",
    "        \n",
    "        target = None\n",
    "        if \"FB-yeild\" in tgt:\n",
    "            bond_data = pd.read_csv(\"data/bond_data.csv\", engine='python', parse_dates=True, infer_datetime_format=True,\n",
    "                       skiprows=range(1,45), index_col=4)\n",
    "            if tgt == 'FB-yeild-1':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5001]\n",
    "            elif tgt == 'FB-yeild-2':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5002]\n",
    "            elif tgt == 'FB-yeild-3':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5003]\n",
    "            elif tgt == 'FB-yeild-4':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5004]\n",
    "            elif tgt == 'FB-yeild-5':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5005]\n",
    "            bond_data.index = bond_data.index.to_period('M') \n",
    "            intersection = bond_data.index.intersection(data.index.to_period('M'))\n",
    "            target = bond_data[bond_data.index.isin(intersection)][\"TMYTM\"]\n",
    "            \n",
    "        return data, transforms, target, train_mask, test_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def factor_standardizer_method(code):\n",
    "        \"\"\"\n",
    "        Outputs the sklearn standard scaler object with the desired features\n",
    "        codes:\n",
    "        0) Identity transform\n",
    "        1) Demean only\n",
    "        2) Demean and standardized\n",
    "        \"\"\"\n",
    "        if code == 0:\n",
    "            return skp.StandardScaler(with_mean=False, with_std=False)\n",
    "        elif code == 1:\n",
    "            return skp.StandardScaler(with_mean=True, with_std=False)\n",
    "        elif code == 2:\n",
    "            return skp.StandardScaler(with_mean=True, with_std=True)\n",
    "        else:\n",
    "            raise ValueError(\"standard_method must be in [0, 1, 2]\")\n",
    "\n",
    "    @staticmethod\n",
    "    def data_transforms(series, transform):\n",
    "        \"\"\"\n",
    "        Transforms a single series according to its transformation code\n",
    "        Inputs:\n",
    "        1) series: pandas series to be transformed\n",
    "        2) transfom: transform code for the series\n",
    "        Returns:\n",
    "        transformed series\n",
    "        \"\"\"\n",
    "        if transform == 1:\n",
    "            # level\n",
    "            return series\n",
    "        elif transform == 2:\n",
    "            # 1st difference\n",
    "            return series.diff()\n",
    "        elif transform == 3:\n",
    "            # second difference\n",
    "            return series.diff().diff()\n",
    "        elif transform == 4:\n",
    "            # Natural log\n",
    "            return np.log(series)\n",
    "        elif transform == 5:\n",
    "            # log 1st difference\n",
    "            return np.log(series).diff()\n",
    "        elif transform == 6:\n",
    "            # log second difference\n",
    "            return np.log(series).diff().diff()\n",
    "        elif transform == 7:\n",
    "            # First difference of percent change\n",
    "            return series.pct_change().diff()\n",
    "        else:\n",
    "            raise ValueError(\"Transform must be in [1, 2, ..., 7]\")\n",
    "\n",
    "    def apply_transforms(self):\n",
    "        \"\"\"\n",
    "        Apply the transformation to each series to make them stationary and drop the first 2 rows that are mostly NaNs\n",
    "        Save results to self.series\n",
    "        \"\"\"\n",
    "        self.series = pd.DataFrame({key: self.data_transforms(\n",
    "            self.rawseries[key], value) for (key, value) in self.transforms.items()})\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        \"\"\"\n",
    "        Removes outliers from each series in self.series\n",
    "        Outlier definition: a data point x of a series X is considered an outlier if abs(x-median)>10*interquartile_range.\n",
    "        \"\"\"\n",
    "        Z = abs((self.series - self.series.median()) /\n",
    "                (self.series.quantile(0.75) - self.series.quantile(0.25))) > 10\n",
    "        for col, _ in self.series.iteritems():\n",
    "            self.series[col][Z[col]] = np.nan\n",
    "        \n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Define our estimation pipelines\n",
    "        self.apply_transforms()\n",
    "        self.remove_outliers()\n",
    "        \n",
    "        self.series = self.series.loc[self.series.index >= '1960-01-01']\n",
    "        self.series = self.series.loc[self.series.index < '2020-01-01']\n",
    "        \n",
    "        pipe = skpipe.Pipeline([('Standardize', self.factor_standardizer_method(self.standard_method))])\n",
    "\n",
    "        actual_data = self.series.to_numpy(copy=True)\n",
    "        intial_nas = self.series.isna().to_numpy(copy=True)\n",
    "        working_data = self.series.fillna(value=self.series.mean(), axis='index').to_numpy(copy=True)\n",
    "\n",
    "        last_timestep = np.sum(self.train_mask) + np.sum(self.test_mask)     \n",
    "\n",
    "        if self.target is None:\n",
    "            idx = self.series.columns.get_loc(self.target_name)\n",
    "            target_data = np.copy(working_data[:,idx])\n",
    "\n",
    "        else:\n",
    "            target_data = self.target\n",
    "            target_data = target_data.loc[target_data.index >= '1960-01-01']\n",
    "            target_data = target_data.loc[target_data.index < '2020-01-01'].to_numpy(copy=True)\n",
    "        \n",
    "        bad_cols = np.isin(self.series.columns.to_numpy(), self.drop_cols)\n",
    "        \n",
    "        return working_data[:-1,~bad_cols], target_data[1:], np.sum(self.train_mask), working_data[:last_timestep]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "nervous-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChenZData:\n",
    "    \n",
    "    def __init__(self, start_date='1976-03', end_date='2019-09', tgt_factor='mkt'):\n",
    "        chen_z_port_data = pd.read_csv(\"data/allportretbase.csv\", engine='python', \n",
    "                                       parse_dates=True, infer_datetime_format=True,\n",
    "                               index_col=0)\n",
    "        chen_z_port_data = chen_z_port_data.loc[chen_z_port_data.date >= start_date]\n",
    "        chen_z_port_data = chen_z_port_data.loc[chen_z_port_data.date < end_date]\n",
    "\n",
    "        chen_z_port_data['port_id'] = chen_z_port_data.signalname.add(chen_z_port_data.port.astype(str))\n",
    "        chen_z_port_data.port_id = chen_z_port_data.port_id\n",
    "        portfolios = sorted(chen_z_port_data.port_id.unique())\n",
    "        dates = chen_z_port_data.date.unique()\n",
    "        portfolios = [p for p in portfolios if \n",
    "                     np.sum(chen_z_port_data['port_id'] == p) == len(dates)]\n",
    "        return_panel = np.zeros((len(dates), len(portfolios)), dtype=float)\n",
    "        for i, pname in enumerate(portfolios):\n",
    "            return_panel[:, i] = chen_z_port_data.loc[chen_z_port_data['port_id'] == pname].ret.to_numpy()\n",
    "        tgt = None\n",
    "        if tgt_factor in {'mkt', 'SMB', 'HML', 'CMA', 'RMW'}:\n",
    "            int_start_date = int(start_date.replace('-', ''))\n",
    "            int_end_date = int(end_date.replace('-', ''))\n",
    "            ff_factors = pd.read_csv('data/F-F_Research_Data_5_Factors_2x3.CSV', skiprows=0,\n",
    "                                       engine='python', parse_dates=True, infer_datetime_format=True)\n",
    "            ff_factors = ff_factors.loc[ff_factors.date >= int_start_date] #NOTE not offsetting by one here across data sources\n",
    "            ff_factors = ff_factors.loc[ff_factors.date < int_end_date]\n",
    "            if tgt_factor == 'mkt':\n",
    "                tgt = ff_factors['Mkt-RF'].to_numpy()\n",
    "            if tgt_factor == 'SMB':\n",
    "                tgt = ff_factors['SMB'].to_numpy()\n",
    "            if tgt_factor == 'HML':\n",
    "                tgt = ff_factors['HML'].to_numpy()\n",
    "            if tgt_factor == 'CMA':\n",
    "                tgt = ff_factors['CMA'].to_numpy()\n",
    "            if tgt_factor == 'RMW':\n",
    "                tgt = ff_factors['RMW'].to_numpy()\n",
    "        self.data = return_panel, tgt, int(tgt.shape[0]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-girlfriend",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-reason",
   "metadata": {},
   "source": [
    "##  SPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "oriental-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, ref=None):\n",
    "    if ref is None:\n",
    "        ref = X\n",
    "    norm = np.std(ref, axis=0, keepdims=True)\n",
    "    return (X - np.mean(ref, axis=0, keepdims=True)) / norm\n",
    "\n",
    "class SPCA:\n",
    "    \n",
    "    def __init__(self, data_panel, target_panel, n_train,\n",
    "                N_factor=6) -> None:\n",
    "        \"\"\"\n",
    "        Auguments:\n",
    "        1) data_panel: numpy 2d array of data, normalizations and transforms should have already been applied\n",
    "        2) target_panel: numpy 2d array of target data, normalizations and transforms should have already been applied\n",
    "        3) n_train: index of start of oos\n",
    "        4) N_factors: number of factors\n",
    "        \"\"\"\n",
    "        self.n_factors = N_factor\n",
    "        self.test_start = n_train\n",
    "        self.data_series = np.copy(data_panel)\n",
    "        self.tgt_series = np.copy(target_panel)\n",
    "        \n",
    "    def fit(self, nlags, true_oos=False, pca=False, raw_data=None,\n",
    "           stack_lags=False, plot_resids=False, fit_factors_epanding_window=True):\n",
    "        T, N = self.data_series.shape\n",
    "        \n",
    "        \n",
    "        \n",
    "        lags = np.zeros((T, nlags))\n",
    "        if stack_lags:\n",
    "            for t in range(T):\n",
    "                lags[t,max(0, nlags-t):] = self.tgt_series[max(0, t-nlags):t]\n",
    "                \n",
    "        # \n",
    "        scaled_data = scale(np.copy(self.data_series[:self.test_start,:]))\n",
    "        scaled_data_full = scale(np.copy(self.data_series), \n",
    "                                 ref=self.data_series[:self.test_start,:])\n",
    "        gamma_is = SPCA.get_gamma_is(scaled_data, self.tgt_series[:self.test_start])\n",
    "        if pca:\n",
    "            gamma_is[:] = 1\n",
    "        gamma_is = np.diag(gamma_is)\n",
    "        _, loadings = SPCA.fit_factors(scaled_data@gamma_is, self.n_factors)\n",
    "\n",
    "        factors = (1/N ) * scaled_data_full @ loadings\n",
    "                    \n",
    "        #In sample R2\n",
    "        A_fit = np.concatenate([factors[:self.test_start], np.ones((self.test_start, 1))], axis=1)\n",
    "        fit_target = self.tgt_series[:self.test_start]\n",
    "        reg_loadings = np.linalg.lstsq(A_fit, fit_target, rcond=None)[0]\n",
    "        preds = A_fit@reg_loadings\n",
    "        is_r2 = np.sum(np.square(preds - fit_target)) / np.sum(np.square(fit_target - np.mean(fit_target)))\n",
    "        print(\"is r2:\", is_r2)\n",
    "        \n",
    "        preds = []\n",
    "        ar_preds = []\n",
    "        gts = []\n",
    "        mean_preds = []\n",
    "        for t in range(self.test_start, self.tgt_series.shape[0]):\n",
    "            if fit_factors_epanding_window:\n",
    "                scaled_data = scale(np.copy(self.data_series[:t,:]))\n",
    "                scaled_data_ext = scale(np.copy(self.data_series[:t+1,:]))\n",
    "                assert np.sum(np.isnan(scaled_data)) == 0 and np.sum(np.isnan(scaled_data_ext)) == 0 \n",
    "                gamma_is = SPCA.get_gamma_is(scaled_data, self.tgt_series[:t])\n",
    "\n",
    "                if pca:\n",
    "                    gamma_is[:] = 1\n",
    "                gamma_is = np.diag(gamma_is)\n",
    "                if true_oos:\n",
    "                    _, loadings = SPCA.fit_factors(scaled_data@gamma_is, self.n_factors)\n",
    "                    factors = (1/N ) * scaled_data_ext @ loadings\n",
    "                    fit_factors = factors[:t]\n",
    "                    test_factors = factors[t:t+1]\n",
    "                else:\n",
    "                    factors, _ = SPCA.fit_factors(scaled_data_ext@gamma_is, self.n_factors, raw_data=raw_data)\n",
    "                    fit_factors = factors[:t]\n",
    "                    test_factors = factors[t:]\n",
    "            else:\n",
    "                fit_factors = factors[:t]\n",
    "                test_factors = factors[t:t+1]\n",
    "                \n",
    "            \n",
    "            if stack_lags:\n",
    "                A_fit = np.concatenate([fit_factors, np.ones((t, 1)), lags[:t]], axis=1)\n",
    "                A_test = np.concatenate([test_factors, np.ones((1, 1)), lags[t:t+1]], axis=1)\n",
    "            else:\n",
    "                A_fit = np.concatenate([fit_factors, np.ones((t, 1))], axis=1)\n",
    "                A_test = np.concatenate([test_factors, np.ones((1, 1))], axis=1)\n",
    "            loadings = np.linalg.lstsq(A_fit, self.tgt_series[:t], rcond=None)[0]\n",
    "\n",
    "            \n",
    "            sim_ar_model = AutoReg(self.tgt_series[:t], lags=nlags, old_names=False,\n",
    "                                  trend='n')\n",
    "            sim_ar_model_fit = sim_ar_model.fit()\n",
    "            \n",
    "            gts.append(self.tgt_series[t])\n",
    "            mean_preds.append(np.mean(self.tgt_series[:t]))\n",
    "            ar_forcast = sim_ar_model_fit.forecast()\n",
    "            ar_preds.append(ar_forcast)\n",
    "            preds.append(A_test@loadings)\n",
    "        preds = np.array(preds).squeeze()\n",
    "        ar_preds = np.array(ar_preds).squeeze()\n",
    "        gts = np.array(gts)\n",
    "        mean_preds = np.array(mean_preds)\n",
    "\n",
    "        if plot_resids:\n",
    "            plt.plot(preds - gts, label='factor resids')\n",
    "            plt.plot(ar_preds - gts, label='ar resids')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"r2 vs ar model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - ar_preds)))\n",
    "        print(\"ar r2 vs mean model\", 1 - np.sum(np.square(gts - ar_preds)) / np.sum(np.square(gts - mean_preds)))\n",
    "        print(\"r2 vs mean model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - mean_preds)))\n",
    "        print(\"EV\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts)))\n",
    "        \n",
    "\n",
    "        \n",
    "    def fit_ts_reg(self, plot_resids=False):\n",
    "        self.test_start\n",
    "        gamma_is = SPCA.get_gamma_is(self.data_series[:self.test_start,:], self.tgt_series[:self.test_start])\n",
    "        factors = SPCA.fit_factors(self.data_series*gamma_is, self.n_factors)[self.test_start:]\n",
    "        loadings = np.linalg.lstsq(factors, self.tgt_series[self.test_start:], rcond=None)[0]\n",
    "        preds = factors.dot(loadings)\n",
    "        gts = self.tgt_series[self.test_start:]\n",
    "        if plot_resids:\n",
    "            plt.plot(preds - gts, label='factor resids')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"r2 vs mean model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - np.mean(gts))))\n",
    "        print(\"EV\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts)))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def fit_factors(scaled_data, n_factors):\n",
    "        T,N = scaled_data.shape\n",
    "\n",
    "        fit_pipe = skpipe.Pipeline([('loadings', skd.TruncatedSVD(n_factors, algorithm='arpack'))])\n",
    "        objective = scaled_data.T.dot(scaled_data)\n",
    "        loadings = fit_pipe.fit_transform(objective)\n",
    "        factors = (1/N ) * scaled_data.dot(loadings)\n",
    "        \n",
    "        return factors, loadings\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gamma_is(data, target):\n",
    "        loadings = []\n",
    "        T = target.shape[0]\n",
    "        \n",
    "        for i in range(data.shape[1]):\n",
    "            A = np.stack([data[:,i], np.ones(T)], axis=1)\n",
    "            loading = np.linalg.lstsq(A, target, rcond=None)[0][0]\n",
    "            loadings.append(loading)\n",
    "        return np.array(loadings)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "incredible-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GX_SPCA:\n",
    "    \n",
    "    def __init__(self, data_panel, target_panel, test_start, N_factors=6):\n",
    "        \"\"\"\n",
    "        Auguments:\n",
    "        1) data_panel: numpy 2d array of data, normalizations and transforms should have already been applied\n",
    "        2) target_panel: numpy 2d array of target data, normalizations and transforms should have already \n",
    "            been applied\n",
    "        3) n_train: index of start of oos\n",
    "        4) N_factors: number of factors\n",
    "        \"\"\"\n",
    "        T,N = data_panel.shape\n",
    "        self.all_tgt = target_panel\n",
    "        self.all_train = scale(data_panel, ref=data_panel[:test_start])\n",
    "        self.train_data = scale(data_panel[:test_start])\n",
    "        self.train_target = target_panel[:test_start]\n",
    "        self.test_data = scale(data_panel[test_start:], ref=data_panel[:test_start])\n",
    "        self.test_target = target_panel[test_start:]\n",
    "        self.n_factors = N_factors\n",
    "        \n",
    "    def fit(self, nlags, true_oos=False, pca=False, raw_data=None,\n",
    "           stack_lags=False, plot_resids=False, quantile=None, print_res=False,\n",
    "           fit_factors_epanding_window=False):\n",
    "        \n",
    "        if quantile is None:\n",
    "            qtile_test_start = int(self.train_data.shape[0]/3)\n",
    "            quantiles = [i*(1/50) for i in range(50)]\n",
    "            qtuile_r2s = [GX_SPCA(self.train_data, self.train_target, \n",
    "                                  qtile_test_start, self.n_factors).fit(nlags=nlags, true_oos=True,\n",
    "                                                                       quantile=q)[0] for q in quantiles]\n",
    "            quantile = quantiles[np.argmax(qtuile_r2s)]\n",
    "            \n",
    "        betas = GX_SPCA.get_portfolio_loadings(self.train_data, self.train_target, self.n_factors,\n",
    "                                              quantile=quantile)\n",
    "        \n",
    "        train_factors = self.train_data @ betas\n",
    "        test_factors = self.test_data @ betas\n",
    "        \n",
    "        #In sample R2\n",
    "        A_fit = np.concatenate([train_factors, np.ones((train_factors.shape[0], 1))], axis=1)\n",
    "        fit_target = self.train_target\n",
    "        loadings = np.linalg.lstsq(A_fit, fit_target, rcond=None)[0]\n",
    "        predicted = A_fit@loadings\n",
    "        is_r2 = np.sum(np.square(predicted - fit_target)) / np.sum(np.square(fit_target - np.mean(fit_target)))\n",
    "        if print_res:\n",
    "            print(\"is r2:\", is_r2)\n",
    "        \n",
    "        preds = []\n",
    "        mean_preds = []\n",
    "        gts = []\n",
    "        for t in range(self.test_data.shape[0]):\n",
    "            if fit_factors_epanding_window and t > 0:\n",
    "                betas = GX_SPCA.get_portfolio_loadings(self.all_train[:t + self.train_target.shape[0]], \n",
    "                                                       self.all_tgt[:t + self.train_target.shape[0]], \n",
    "                                                       self.n_factors,\n",
    "                                                       quantile=quantile)\n",
    "                \n",
    "                train_factors = self.train_data @ betas\n",
    "                test_factors = self.test_data @ betas\n",
    "            \n",
    "            if t > 0:\n",
    "                fit_factors = np.concatenate([train_factors, test_factors[:t]], axis=0)\n",
    "                fit_target = np.concatenate([self.train_target, self.test_target[:t]], axis=0)\n",
    "            else:\n",
    "                fit_factors = train_factors\n",
    "                fit_target = self.train_target\n",
    "            \n",
    "            eval_factors = test_factors[t:t+1]\n",
    "            \n",
    "            A_fit = np.concatenate([fit_factors, np.ones((fit_factors.shape[0], 1))], axis=1)\n",
    "            A_test = np.concatenate([eval_factors, np.ones((1, 1))], axis=1)\n",
    "            \n",
    "            loadings = np.linalg.lstsq(A_fit, fit_target, rcond=None)[0]\n",
    "        \n",
    "            gts.append(self.test_target[t])\n",
    "            mean_preds.append(np.mean(self.all_tgt[:t + self.train_target.shape[0]]))\n",
    "        \n",
    "            preds.append(A_test@loadings)\n",
    "\n",
    "        preds = np.array(preds).squeeze()\n",
    "        mean_preds = np.array(mean_preds)\n",
    "        gts = np.array(gts)\n",
    "        r2 =  1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - mean_preds))\n",
    "        ev = 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts))\n",
    "        if print_res:\n",
    "            print(\"r2 vs mean model\", r2)\n",
    "            print(\"EV\", ev)\n",
    "        else:\n",
    "            return is_r2, r2, ev\n",
    "        \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_portfolio_loadings(train_data, train_tgt, nfactor, quantile):\n",
    "        T, N = train_data.shape\n",
    "        loadings = np.zeros((N, nfactor))\n",
    "        \n",
    "        R_k = np.copy(train_data.T)\n",
    "        G_k = np.copy(train_tgt.T)\n",
    "        rbar_k = np.mean(R_k, axis=1)\n",
    "        \n",
    "        eta_hats = []\n",
    "        gamma_hats = []\n",
    "        v_hats = []\n",
    "        beta_hats = []\n",
    "        \n",
    "        for k in range(nfactor):\n",
    "            vhat, gamma_hat, eta_hat, beta_hat, R_k, rbar_k, G_k = GX_SPCA.get_next_factor_loading(R_k, G_k, rbar_k,\n",
    "                                                                                                  quantile=quantile)\n",
    "            eta_hats.append(eta_hat)\n",
    "            gamma_hats.append(gamma_hat)\n",
    "            v_hats.append(vhat)\n",
    "            beta_hats.append(beta_hat)\n",
    "        \n",
    "        betas = np.concatenate(beta_hats, axis=1)\n",
    "        return betas\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_next_factor_loading(train_data, train_tgt, rbar, quantile):\n",
    "        #train_data: n times T\n",
    "        #train_tgt: 1 times T\n",
    "        N,T = train_data.shape \n",
    "        cors = (1/T)*(train_data @ train_tgt.T)\n",
    "        thresh = np.quantile(cors, quantile) # FIXME\n",
    "        selected_inds = np.argwhere(cors >= thresh).squeeze()\n",
    "#         print(cors.shape, selected_inds.shape)\n",
    "        selected_data = train_data[selected_inds,:] #\n",
    "        \n",
    "        # algo 1\n",
    "        psi, singval, xi = randomized_svd(selected_data, n_components=1,\n",
    "                                           n_iter=10, random_state=None)\n",
    "        vhat = np.sqrt(T) * xi #factors\n",
    "        gamma_hat = (singval/T)*psi.T @ rbar[selected_inds]\n",
    "        eta_hat = (1/T) * train_tgt @ vhat.T\n",
    "        #END algo 1\n",
    "        \n",
    "        beta_hat = (1/T) * train_data @ vhat.T\n",
    "        \n",
    "        train_data_perp = train_data - beta_hat @ vhat\n",
    "        rbar_perp = rbar - beta_hat.squeeze() * gamma_hat\n",
    "        target_perp = train_tgt - eta_hat * vhat.squeeze()\n",
    "        \n",
    "        return vhat, gamma_hat, eta_hat, beta_hat, train_data_perp, rbar_perp, target_perp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-double",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-copper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "disciplinary-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "cloudy-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lags(target_series_train, max_lags):\n",
    "    bics = []\n",
    "    for i in range(max_lags):\n",
    "        i += 1\n",
    "        model = AutoReg(target_series_train, lags=i, old_names=False, trend='n')\n",
    "        model_fit = model.fit()\n",
    "        bics.append(model_fit.bic)\n",
    "    return np.argmax(bics) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "processed-killer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VXOCLSx 1\n",
      "SPCA\n",
      "is r2: 0.7085049492535421\n",
      "r2 vs ar model -2.5139666147096356\n",
      "ar r2 vs mean model 0.77591764844881\n",
      "r2 vs mean model 0.21258209770349001\n",
      "EV 0.8840031842719243\n",
      "PCA\n",
      "is r2: 0.7791087292486113\n",
      "r2 vs ar model -3.017413181746897\n",
      "ar r2 vs mean model 0.77591764844881\n",
      "r2 vs mean model 0.09976860708140667\n",
      "EV 0.8673843016618573\n",
      "giglio xui\n",
      "is r2: 0.7977254529473414\n",
      "r2 vs mean model 0.12019280880567207\n",
      "EV 0.8703930500747307\n",
      "\n",
      "UNRATE 1\n",
      "SPCA\n",
      "is r2: 0.7208798651997318\n",
      "r2 vs ar model 0.17136344902024536\n",
      "ar r2 vs mean model 0.003061115352460897\n",
      "r2 vs mean model 0.1739000010880597\n",
      "EV 0.17126251458179143\n",
      "PCA\n",
      "is r2: 0.736258995746545\n",
      "r2 vs ar model 0.11190827681038173\n",
      "ar r2 vs mean model 0.003061115352460897\n",
      "r2 vs mean model 0.11462682801863089\n",
      "EV 0.11180010026510412\n",
      "giglio xui\n",
      "is r2: 0.767426223764381\n",
      "r2 vs mean model 0.0038285768101509188\n",
      "EV 0.00064810387702674\n",
      "\n",
      "INDPRO 5\n",
      "SPCA\n",
      "is r2: 0.708835098011172\n",
      "r2 vs ar model -0.033832142694584455\n",
      "ar r2 vs mean model 0.11179035309279373\n",
      "r2 vs mean model 0.0817403175759227\n",
      "EV 0.1260453852040766\n",
      "PCA\n",
      "is r2: 0.7141897483591635\n",
      "r2 vs ar model -0.007817496152193959\n",
      "ar r2 vs mean model 0.11179035309279373\n",
      "r2 vs mean model 0.10484677759575511\n",
      "EV 0.14803698273629184\n",
      "giglio xui\n",
      "is r2: 0.6952429612787439\n",
      "r2 vs mean model 0.07796351424083026\n",
      "EV 0.1224508086729943\n",
      "\n",
      "CPIAUCSL 1\n",
      "SPCA\n",
      "is r2: 0.7597351445798772\n",
      "r2 vs ar model 0.08402228998085093\n",
      "ar r2 vs mean model -0.01942888224606132\n",
      "r2 vs mean model 0.06622586691287213\n",
      "EV 0.06439366521393075\n",
      "PCA\n",
      "is r2: 0.8613284982176086\n",
      "r2 vs ar model -0.043778624373201014\n",
      "ar r2 vs mean model -0.01942888224606132\n",
      "r2 vs mean model -0.0640580763571037\n",
      "EV -0.06614591424658167\n",
      "giglio xui\n",
      "is r2: 0.9893929421752689\n",
      "r2 vs mean model -0.0074570875409532\n",
      "EV -0.009433865995182256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tgt in [\"VXOCLSx\", \"UNRATE\", \"INDPRO\", \"CPIAUCSL\"]:\n",
    "    spca_data = SPCAData(target=tgt, \n",
    "                         drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]\n",
    "#                          drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\"]\n",
    "                        )\n",
    "    data_panel, tgt_data, test_start, raw_data = spca_data.get_data()\n",
    "    opt_lags = get_lags(tgt_data[:test_start], max_lags=5)\n",
    "    print(tgt, opt_lags)\n",
    "    spca = SPCA(data_panel, tgt_data, test_start, N_factor=4)\n",
    "    print(\"SPCA\")\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, fit_factors_epanding_window=True)\n",
    "    print(\"PCA\")\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, pca=True, fit_factors_epanding_window=True)\n",
    "    print(\"giglio xui\")\n",
    "    spca = GX_SPCA(data_panel, tgt_data, test_start, N_factors=4)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, print_res=True, fit_factors_epanding_window=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "auburn-fantasy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB-yeild-1 4\n",
      "is r2: 0.34018291226984765\n",
      "r2 vs ar model -133.8502482058935\n",
      "ar r2 vs mean model 0.9942046744466398\n",
      "r2 vs mean model 0.21849891069542626\n",
      "EV 0.5737199299702239\n",
      "is r2: 0.7230648305605135\n",
      "r2 vs ar model -132.90968282372003\n",
      "ar r2 vs mean model 0.9942046744466398\n",
      "r2 vs mean model 0.22394979328934217\n",
      "EV 0.5766931857284804\n",
      "giglio xui\n",
      "is r2: 0.8736392676469477\n",
      "r2 vs mean model 0.06980615686125025\n",
      "EV 0.4926135074906899\n",
      "\n",
      "FB-yeild-2 5\n",
      "is r2: 0.3681442768242849\n",
      "r2 vs ar model -121.96975569462843\n",
      "ar r2 vs mean model 0.9930864643215482\n",
      "r2 vs mean model 0.14984420663469367\n",
      "EV 0.591802473948146\n",
      "is r2: 0.7824238249656462\n",
      "r2 vs ar model -118.43739651310442\n",
      "ar r2 vs mean model 0.9930864643215482\n",
      "r2 vs mean model 0.17426529786526646\n",
      "EV 0.6035281236485921\n",
      "giglio xui\n",
      "is r2: 0.8968141643153564\n",
      "r2 vs mean model 0.054661570149194594\n",
      "EV 0.5461010659948172\n",
      "\n",
      "FB-yeild-3 5\n",
      "is r2: 0.33707553413050606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-833-2bb2bfbb0f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_lags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mspca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_panel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mspca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_oos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_lags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_factors_epanding_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mspca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_oos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_factors_epanding_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"giglio xui\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-827-b28cb3bcbc37>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, nlags, true_oos, pca, raw_data, stack_lags, plot_resids, fit_factors_epanding_window)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             sim_ar_model = AutoReg(self.tgt_series[:t], lags=nlags, old_names=False,\n\u001b[0;32m---> 92\u001b[0;31m                                   trend='n')\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0msim_ar_model_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_ar_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/statsmodels/tsa/ar_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, lags, trend, seasonal, exog, hold_back, period, missing, deterministic, old_names)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_lags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_regressors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/statsmodels/tsa/ar_model.py\u001b[0m in \u001b[0;36m_setup_regressors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lags\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_k_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/statsmodels/tsa/deterministic.py\u001b[0m in \u001b[0;36min_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mraw_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic_terms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mraw_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mraw_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adjust_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/statsmodels/tsa/deterministic.py\u001b[0m in \u001b[0;36min_sample\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnobs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeterministicTerm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_of_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env-349/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tgt = f'FB-yeild-{i+1}'\n",
    "    spca_data = SPCAData(target=tgt, \n",
    "                         drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]\n",
    "#                          drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\"]\n",
    "                        )\n",
    "    data_panel, tgt_data, test_start, raw_data = spca_data.get_data()\n",
    "    opt_lags = get_lags(tgt_data[:test_start], max_lags=5)\n",
    "    print(tgt, opt_lags)\n",
    "    spca = SPCA(data_panel, tgt_data, test_start, N_factor=5)\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, fit_factors_epanding_window=False)\n",
    "    spca.fit(opt_lags, true_oos=True, pca=True, fit_factors_epanding_window=False)\n",
    "    print(\"giglio xui\")\n",
    "    spca = GX_SPCA(data_panel, tgt_data, test_start, N_factors=5)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, print_res=True, fit_factors_epanding_window=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-deployment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "deluxe-pacific",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkt\n",
      "spca\n",
      "is r2: 0.012614124066276839\n",
      "r2 vs ar model 0.9718509840151561\n",
      "ar r2 vs mean model -0.008956470002243533\n",
      "r2 vs mean model 0.9715988681978952\n",
      "EV 0.9719244931250737\n",
      "pca\n",
      "is r2: 0.01551556351788213\n",
      "r2 vs ar model 0.9644800742586304\n",
      "ar r2 vs mean model -0.008956470002243533\n",
      "r2 vs mean model 0.9641619411092459\n",
      "EV 0.9645728319638017\n",
      "gx\n",
      "is r2: 0.016001418016670686\n",
      "r2 vs mean model 0.9475370577616697\n",
      "EV 0.9481385563873217\n",
      "\n",
      "SMB\n",
      "spca\n",
      "is r2: 0.33218597951784756\n",
      "r2 vs ar model 0.49062316852052357\n",
      "ar r2 vs mean model -0.03806164735615991\n",
      "r2 vs mean model 0.4712354471893536\n",
      "EV 0.47138946206509436\n",
      "pca\n",
      "is r2: 0.12360302984378417\n",
      "r2 vs ar model 0.7212726830207307\n",
      "ar r2 vs mean model -0.03806164735615991\n",
      "r2 vs mean model 0.710663862173337\n",
      "EV 0.7107481379990856\n",
      "gx\n",
      "is r2: 0.4229352285059945\n",
      "r2 vs mean model 0.5243967320680396\n",
      "EV 0.5245352624930144\n",
      "\n",
      "HML\n",
      "spca\n",
      "is r2: 0.27586590642717107\n",
      "r2 vs ar model 0.5842041191457537\n",
      "ar r2 vs mean model 0.021473849942505452\n",
      "r2 vs mean model 0.5931328574979295\n",
      "EV 0.5898848828442589\n",
      "pca\n",
      "is r2: 0.2750423028563211\n",
      "r2 vs ar model 0.5792079365724607\n",
      "ar r2 vs mean model 0.021473849942505452\n",
      "r2 vs mean model 0.588243962199501\n",
      "EV 0.5849569600443316\n",
      "gx\n",
      "is r2: 0.39131167980730286\n",
      "r2 vs mean model 0.5058389331006103\n",
      "EV 0.5018941008631144\n",
      "\n",
      "CMA\n",
      "spca\n",
      "is r2: 0.4299198731767217\n",
      "r2 vs ar model 0.4332899725139313\n",
      "ar r2 vs mean model -0.02315126629263431\n",
      "r2 vs mean model 0.4201699177568953\n",
      "EV 0.4236915103105795\n",
      "pca\n",
      "is r2: 0.42416693466553906\n",
      "r2 vs ar model 0.36095846012757726\n",
      "ar r2 vs mean model -0.02315126629263431\n",
      "r2 vs mean model 0.34616383926593575\n",
      "EV 0.35013490704162464\n",
      "gx\n",
      "is r2: 0.4305895714479441\n",
      "r2 vs mean model 0.22946301340992004\n",
      "EV 0.2341428625543739\n",
      "\n",
      "RMW\n",
      "spca\n",
      "is r2: 0.7647785913872817\n",
      "r2 vs ar model 0.5590691836042849\n",
      "ar r2 vs mean model -0.059931930246706955\n",
      "r2 vs mean model 0.5326433486724333\n",
      "EV 0.5359255806539731\n",
      "pca\n",
      "is r2: 0.7782846373602524\n",
      "r2 vs ar model 0.587445060457986\n",
      "ar r2 vs mean model -0.059931930246706955\n",
      "r2 vs mean model 0.5627198465984196\n",
      "EV 0.5657908522218772\n",
      "gx\n",
      "is r2: 0.7888763986759985\n",
      "r2 vs mean model 0.5336610969235906\n",
      "EV 0.5369361812891724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tgt in 'mkt', 'SMB', 'HML', 'CMA', 'RMW' :\n",
    "    print(tgt)\n",
    "    chen_z_data_panel, tgt_factor, weak_test_start = ChenZData(tgt_factor=tgt).data\n",
    "    spca = SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factor=5)\n",
    "    print(\"spca\")\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=True)\n",
    "    print(\"pca\")\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=True,\n",
    "            pca=True)\n",
    "    print(\"gx\")\n",
    "    spca = GX_SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factors=5)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=True, print_res=True)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "tender-edward",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkt\n",
      "spca\n",
      "is r2: 0.0126141240662768\n",
      "r2 vs ar model 0.9729738598080271\n",
      "ar r2 vs mean model -0.008956470002243533\n",
      "r2 vs mean model 0.9727318009941213\n",
      "EV 0.9730444366094005\n",
      "pca\n",
      "is r2: 0.015515563517882042\n",
      "r2 vs ar model 0.9691723205840495\n",
      "ar r2 vs mean model -0.008956470002243533\n",
      "r2 vs mean model 0.9688962133981217\n",
      "EV 0.9692528248288838\n",
      "gx\n",
      "is r2: 0.014972537304636156\n",
      "r2 vs mean model 0.9708260092003416\n",
      "EV 0.9711604951178691\n",
      "\n",
      "SMB\n",
      "spca\n",
      "is r2: 0.3321859795178906\n",
      "r2 vs ar model 0.5125360009809733\n",
      "ar r2 vs mean model -0.03806164735615991\n",
      "r2 vs mean model 0.49398231815148763\n",
      "EV 0.4941297074762414\n",
      "pca\n",
      "is r2: 0.12360302984378978\n",
      "r2 vs ar model 0.74536102149714\n",
      "ar r2 vs mean model -0.03806164735615991\n",
      "r2 vs mean model 0.7356690424942314\n",
      "EV 0.7357460349842881\n",
      "gx\n",
      "is r2: 0.42293522850726134\n",
      "r2 vs mean model 0.33838857084242513\n",
      "EV 0.3385812804359649\n",
      "\n",
      "HML\n",
      "spca\n",
      "is r2: 0.27586590642717174\n",
      "r2 vs ar model 0.627852320996273\n",
      "ar r2 vs mean model 0.021473849942505452\n",
      "r2 vs mean model 0.6358437644116508\n",
      "EV 0.6329367461257958\n",
      "pca\n",
      "is r2: 0.2750423028563212\n",
      "r2 vs ar model 0.6323927911320947\n",
      "ar r2 vs mean model 0.021473849942505452\n",
      "r2 vs mean model 0.6402867331731074\n",
      "EV 0.6374151826073429\n",
      "gx\n",
      "is r2: 0.3797592208595892\n",
      "r2 vs mean model 0.6125846593186652\n",
      "EV 0.6094919662119228\n",
      "\n",
      "CMA\n",
      "spca\n",
      "is r2: 0.42991987317672115\n",
      "r2 vs ar model 0.39060126585728905\n",
      "ar r2 vs mean model -0.02315126629263431\n",
      "r2 vs mean model 0.3764929134847569\n",
      "EV 0.3802797779132928\n",
      "pca\n",
      "is r2: 0.4241669346655404\n",
      "r2 vs ar model 0.3883657169881245\n",
      "ar r2 vs mean model -0.02315126629263431\n",
      "r2 vs mean model 0.37420560882841214\n",
      "EV 0.3780063651802761\n",
      "gx\n",
      "is r2: 0.4468719828308374\n",
      "r2 vs mean model 0.39845608375273\n",
      "EV 0.40210955507311297\n",
      "\n",
      "RMW\n",
      "spca\n",
      "is r2: 0.7647785913872783\n",
      "r2 vs ar model 0.5322118396363874\n",
      "ar r2 vs mean model -0.059931930246706955\n",
      "r2 vs mean model 0.50417639223924\n",
      "EV 0.5076585468164179\n",
      "pca\n",
      "is r2: 0.7782846373602462\n",
      "r2 vs ar model 0.5617958205547869\n",
      "ar r2 vs mean model -0.059931930246706955\n",
      "r2 vs mean model 0.5355333982384609\n",
      "EV 0.5387953334871161\n",
      "gx\n",
      "is r2: 0.7888763986759989\n",
      "r2 vs mean model 0.5178522380802265\n",
      "EV 0.5212383475953166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tgt in 'mkt', 'SMB', 'HML', 'CMA', 'RMW' :\n",
    "    print(tgt)\n",
    "    chen_z_data_panel, tgt_factor, weak_test_start = ChenZData(tgt_factor=tgt).data\n",
    "    spca = SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factor=5)\n",
    "    print(\"spca\")\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=False)\n",
    "    print(\"pca\")\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=False,\n",
    "            pca=True)\n",
    "    print(\"gx\")\n",
    "    spca = GX_SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factors=5)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, fit_factors_epanding_window=False, print_res=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-tulsa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
