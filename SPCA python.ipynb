{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "overall-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fredmd import FredMD\n",
    "import sklearn.pipeline as skpipe\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.preprocessing as skp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-message",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "protected-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPCAData:\n",
    "    \n",
    "    def __init__(self, Nfactor=None, vintage=None, maxfactor=8, standard_method=2, ic_method=2,\n",
    "                 target=None, train_test_split=[('1960-01-01', '1984-12-01'),('1985-01-01', '2019-12-01')], \n",
    "                 nlags=1, drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]) -> None:\n",
    "        \"\"\"\n",
    "        Create fredmd object\n",
    "        Auguments:\n",
    "        1) Nfactor = None: Number of factors to estimate. If None then estimate number of true factors via information critea\n",
    "        2) vintage = None: Vinatege of data to use in \"year-month\" format (e.g. \"2020-10\"). If None use current vintage\n",
    "        3) maxfactor = 8: Maximimum number of factors to test against information critea. If Nfactor is a number, then this is ignored\n",
    "        4) standard_method = 2: method to standardize data before factors are estimate. 0 = Identity transform, 1 = Demean only, 2 = Demean and stardize to unit variance. Default = 2.\n",
    "        5) ic_method = 2: information critea penalty term. Se\n",
    "        e http://www.columbia.edu/~sn2294/pub/ecta02.pdf page 201, equation 9 for options.\n",
    "        \"\"\"\n",
    "        self.drop_cols = drop_cols\n",
    "        self.train_test_split = [[datetime.strptime(x, '%Y-%m-%d') for x in split] for split in train_test_split]\n",
    "        # Make sure arguments are valid\n",
    "        if standard_method not in [0, 1, 2]:\n",
    "            raise ValueError(f\"standard_method must be in [0, 1, 2], got {standard_method}\")\n",
    "        if ic_method not in [1, 2, 3]:\n",
    "            raise ValueError(f\"ic_method must be in [1, 2, 3], got {ic_method}\")\n",
    "        # Download data\n",
    "        self.rawseries, self.transforms, self.target, self.train_mask, self.test_mask = self.download_data(vintage, \n",
    "                                                                       target if target is not None else \"UNRATE\",\n",
    "                                                                         self.train_test_split)\n",
    "        \n",
    "\n",
    "        self.target_name = target\n",
    "        self.standard_method = standard_method\n",
    "        self.ic_method = ic_method\n",
    "        self.maxfactor = maxfactor\n",
    "        self.Nfactor = Nfactor\n",
    "\n",
    "        self.nlags = nlags\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data(vintage, tgt, train_test_split):\n",
    "        if vintage is None:\n",
    "            url = 'https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/monthly/current.csv'\n",
    "        else:\n",
    "            url = f'https://s3.amazonaws.com/files.fred.stlouisfed.org/fred-md/monthly/{vintage}.csv'\n",
    "        transforms = pd.read_csv(\n",
    "            url, header=0, nrows=1, index_col=0).transpose()\n",
    "        transforms.index.rename(\"series\", inplace=True)\n",
    "        transforms.columns = ['transform']\n",
    "        transforms = transforms.to_dict()['transform']\n",
    "        data = pd.read_csv(url, names=transforms.keys(), skiprows=2, index_col=0,\n",
    "                           skipfooter=1, engine='python', parse_dates=True, infer_datetime_format=True)\n",
    "        \n",
    "        train_mask = (data.index > train_test_split[0][0]) & (data.index <= train_test_split[0][1])\n",
    "        test_mask = (data.index > train_test_split[1][0]) & (data.index <= train_test_split[1][1])\n",
    "        \n",
    "        target = None\n",
    "        if \"FB-yeild\" in tgt:\n",
    "            bond_data = pd.read_csv(\"data/bond_data.csv\", engine='python', parse_dates=True, infer_datetime_format=True,\n",
    "                       skiprows=range(1,45), index_col=4)\n",
    "            if tgt == 'FB-yeild-1':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5001]\n",
    "            elif tgt == 'FB-yeild-2':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5002]\n",
    "            elif tgt == 'FB-yeild-3':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5003]\n",
    "            elif tgt == 'FB-yeild-4':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5004]\n",
    "            elif tgt == 'FB-yeild-5':\n",
    "                bond_data = bond_data.loc[bond_data['TTERMTYPE'] == 5005]\n",
    "            bond_data.index = bond_data.index.to_period('M') \n",
    "            intersection = bond_data.index.intersection(data.index.to_period('M'))\n",
    "            target = bond_data[bond_data.index.isin(intersection)][\"TMYTM\"]\n",
    "            \n",
    "        return data, transforms, target, train_mask, test_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def factor_standardizer_method(code):\n",
    "        \"\"\"\n",
    "        Outputs the sklearn standard scaler object with the desired features\n",
    "        codes:\n",
    "        0) Identity transform\n",
    "        1) Demean only\n",
    "        2) Demean and standardized\n",
    "        \"\"\"\n",
    "        if code == 0:\n",
    "            return skp.StandardScaler(with_mean=False, with_std=False)\n",
    "        elif code == 1:\n",
    "            return skp.StandardScaler(with_mean=True, with_std=False)\n",
    "        elif code == 2:\n",
    "            return skp.StandardScaler(with_mean=True, with_std=True)\n",
    "        else:\n",
    "            raise ValueError(\"standard_method must be in [0, 1, 2]\")\n",
    "\n",
    "    @staticmethod\n",
    "    def data_transforms(series, transform):\n",
    "        \"\"\"\n",
    "        Transforms a single series according to its transformation code\n",
    "        Inputs:\n",
    "        1) series: pandas series to be transformed\n",
    "        2) transfom: transform code for the series\n",
    "        Returns:\n",
    "        transformed series\n",
    "        \"\"\"\n",
    "        if transform == 1:\n",
    "            # level\n",
    "            return series\n",
    "        elif transform == 2:\n",
    "            # 1st difference\n",
    "            return series.diff()\n",
    "        elif transform == 3:\n",
    "            # second difference\n",
    "            return series.diff().diff()\n",
    "        elif transform == 4:\n",
    "            # Natural log\n",
    "            return np.log(series)\n",
    "        elif transform == 5:\n",
    "            # log 1st difference\n",
    "            return np.log(series).diff()\n",
    "        elif transform == 6:\n",
    "            # log second difference\n",
    "            return np.log(series).diff().diff()\n",
    "        elif transform == 7:\n",
    "            # First difference of percent change\n",
    "            return series.pct_change().diff()\n",
    "        else:\n",
    "            raise ValueError(\"Transform must be in [1, 2, ..., 7]\")\n",
    "\n",
    "    def apply_transforms(self):\n",
    "        \"\"\"\n",
    "        Apply the transformation to each series to make them stationary and drop the first 2 rows that are mostly NaNs\n",
    "        Save results to self.series\n",
    "        \"\"\"\n",
    "        self.series = pd.DataFrame({key: self.data_transforms(\n",
    "            self.rawseries[key], value) for (key, value) in self.transforms.items()})\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        \"\"\"\n",
    "        Removes outliers from each series in self.series\n",
    "        Outlier definition: a data point x of a series X is considered an outlier if abs(x-median)>10*interquartile_range.\n",
    "        \"\"\"\n",
    "        Z = abs((self.series - self.series.median()) /\n",
    "                (self.series.quantile(0.75) - self.series.quantile(0.25))) > 10\n",
    "        for col, _ in self.series.iteritems():\n",
    "            self.series[col][Z[col]] = np.nan\n",
    "        \n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Define our estimation pipelines\n",
    "        self.apply_transforms()\n",
    "        self.remove_outliers()\n",
    "        \n",
    "        self.series = self.series.loc[self.series.index >= '1960-01-01']\n",
    "        self.series = self.series.loc[self.series.index < '2020-01-01']\n",
    "        \n",
    "        pipe = skpipe.Pipeline([('Standardize', self.factor_standardizer_method(self.standard_method))])\n",
    "\n",
    "        actual_data = self.series.to_numpy(copy=True)\n",
    "        intial_nas = self.series.isna().to_numpy(copy=True)\n",
    "        working_data = self.series.fillna(value=self.series.mean(), axis='index').to_numpy(copy=True)\n",
    "\n",
    "        last_timestep = np.sum(self.train_mask) + np.sum(self.test_mask)     \n",
    "\n",
    "        if self.target is None:\n",
    "            idx = self.series.columns.get_loc(self.target_name)\n",
    "            target_data = np.copy(working_data[:,idx])\n",
    "\n",
    "        else:\n",
    "            target_data = self.target\n",
    "            target_data = target_data.loc[target_data.index >= '1960-01-01']\n",
    "            target_data = target_data.loc[target_data.index < '2020-01-01'].to_numpy(copy=True)\n",
    "        \n",
    "        bad_cols = np.isin(self.series.columns.to_numpy(), self.drop_cols)\n",
    "        \n",
    "        return working_data[:-1,~bad_cols], target_data[1:], np.sum(self.train_mask), working_data[:last_timestep]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "absolute-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChenZData:\n",
    "    \n",
    "    def __init__(self, start_date='1976-03', end_date='2019-09', tgt_factor='mkt'):\n",
    "        chen_z_port_data = pd.read_csv(\"data/allportretbase.csv\", engine='python', \n",
    "                                       parse_dates=True, infer_datetime_format=True,\n",
    "                               index_col=0)\n",
    "        chen_z_port_data = chen_z_port_data.loc[chen_z_port_data.date >= start_date]\n",
    "        chen_z_port_data = chen_z_port_data.loc[chen_z_port_data.date < end_date]\n",
    "\n",
    "        chen_z_port_data['port_id'] = chen_z_port_data.signalname.add(chen_z_port_data.port.astype(str))\n",
    "        chen_z_port_data.port_id = chen_z_port_data.port_id\n",
    "        portfolios = sorted(chen_z_port_data.port_id.unique())\n",
    "        dates = chen_z_port_data.date.unique()\n",
    "        portfolios = [p for p in portfolios if \n",
    "                     np.sum(chen_z_port_data['port_id'] == p) == len(dates)]\n",
    "        return_panel = np.zeros((len(dates), len(portfolios)), dtype=float)\n",
    "        for i, pname in enumerate(portfolios):\n",
    "            return_panel[:, i] = chen_z_port_data.loc[chen_z_port_data['port_id'] == pname].ret.to_numpy()\n",
    "        tgt = None\n",
    "        if tgt_factor in {'mkt', 'SMB', 'HML', 'CMA', 'RMW'}:\n",
    "            int_start_date = int(start_date.replace('-', ''))\n",
    "            int_end_date = int(end_date.replace('-', ''))\n",
    "            ff_factors = pd.read_csv('data/F-F_Research_Data_5_Factors_2x3.CSV', skiprows=0,\n",
    "                                       engine='python', parse_dates=True, infer_datetime_format=True)\n",
    "            ff_factors = ff_factors.loc[ff_factors.date >= int_start_date] #NOTE not offsetting by one here across data sources\n",
    "            ff_factors = ff_factors.loc[ff_factors.date < int_end_date]\n",
    "            if tgt_factor == 'mkt':\n",
    "                tgt = ff_factors['Mkt-RF'].to_numpy()\n",
    "            if tgt_factor == 'SMB':\n",
    "                tgt = ff_factors['SMB'].to_numpy()\n",
    "            if tgt_factor == 'HML':\n",
    "                tgt = ff_factors['HML'].to_numpy()\n",
    "            if tgt_factor == 'CMA':\n",
    "                tgt = ff_factors['CMA'].to_numpy()\n",
    "            if tgt_factor == 'RMW':\n",
    "                tgt = ff_factors['RMW'].to_numpy()\n",
    "        self.data = return_panel, tgt, int(tgt.shape[0]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-large",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-holiday",
   "metadata": {},
   "source": [
    "##  SPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "dangerous-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, ref=None):\n",
    "    if ref is None:\n",
    "        ref = X\n",
    "    norm = np.std(ref, axis=0, keepdims=True)\n",
    "    return (X - np.mean(ref, axis=0, keepdims=True)) / norm\n",
    "\n",
    "class SPCA:\n",
    "    \n",
    "    def __init__(self, data_panel, target_panel, n_train,\n",
    "                N_factor=6) -> None:\n",
    "        \"\"\"\n",
    "        Auguments:\n",
    "        1) data_panel: numpy 2d array of data, normalizations and transforms should have already been applied\n",
    "        2) target_panel: numpy 2d array of target data, normalizations and transforms should have already been applied\n",
    "        3) n_train: index of start of oos\n",
    "        4) N_factors: number of factors\n",
    "        \"\"\"\n",
    "        self.n_factors = N_factor\n",
    "        self.test_start = n_train\n",
    "        self.data_series = np.copy(data_panel)\n",
    "        self.tgt_series = np.copy(target_panel)\n",
    "        \n",
    "    def fit(self, nlags, true_oos=False, pca=False, raw_data=None,\n",
    "           stack_lags=False, plot_resids=False, fit_factors_epanding_window=True):\n",
    "        T, N = self.data_series.shape\n",
    "        preds = []\n",
    "        ar_preds = []\n",
    "        gts = []\n",
    "        mean_preds = []\n",
    "        \n",
    "        \n",
    "        lags = np.zeros((T, nlags))\n",
    "        if stack_lags:\n",
    "            for t in range(T):\n",
    "                lags[t,max(0, nlags-t):] = self.tgt_series[max(0, t-nlags):t]\n",
    "                \n",
    "        if not fit_factors_epanding_window:\n",
    "            scaled_data = scale(np.copy(self.data_series[:self.test_start,:]))\n",
    "            scaled_data_full = scale(np.copy(self.data_series), \n",
    "                                     ref=self.data_series[:self.test_start,:])\n",
    "            gamma_is = SPCA.get_gamma_is(scaled_data, self.tgt_series[:self.test_start])\n",
    "            if pca:\n",
    "                gamma_is[:] = 1\n",
    "            gamma_is = np.diag(gamma_is)\n",
    "            _, loadings = SPCA.fit_factors(scaled_data@gamma_is, self.n_factors)\n",
    "            \n",
    "            factors = (1/N ) * scaled_data_full @ loadings\n",
    "            \n",
    "            \n",
    "        for t in range(self.test_start, self.tgt_series.shape[0]):\n",
    "            if fit_factors_epanding_window:\n",
    "                scaled_data = scale(np.copy(self.data_series[:t,:]))\n",
    "                scaled_data_ext = scale(np.copy(self.data_series[:t+1,:]))\n",
    "                assert np.sum(np.isnan(scaled_data)) == 0 and np.sum(np.isnan(scaled_data_ext)) == 0 \n",
    "                gamma_is = SPCA.get_gamma_is(scaled_data, self.tgt_series[:t])\n",
    "\n",
    "                if pca:\n",
    "                    gamma_is[:] = 1\n",
    "                gamma_is = np.diag(gamma_is)\n",
    "                if true_oos:\n",
    "                    _, loadings = SPCA.fit_factors(scaled_data@gamma_is, self.n_factors)\n",
    "                    factors = (1/N ) * scaled_data_ext @ loadings\n",
    "                    fit_factors = factors[:t]\n",
    "                    test_factors = factors[t:t+1]\n",
    "                else:\n",
    "                    factors, _ = SPCA.fit_factors(scaled_data_ext@gamma_is, self.n_factors, raw_data=raw_data)\n",
    "                    fit_factors = factors[:t]\n",
    "                    test_factors = factors[t:]\n",
    "            else:\n",
    "                fit_factors = factors[:t]\n",
    "                test_factors = factors[t:t+1]\n",
    "                \n",
    "            \n",
    "            if stack_lags:\n",
    "                A_fit = np.concatenate([fit_factors, np.ones((t, 1)), lags[:t]], axis=1)\n",
    "                A_test = np.concatenate([test_factors, np.ones((1, 1)), lags[t:t+1]], axis=1)\n",
    "            else:\n",
    "                A_fit = np.concatenate([fit_factors, np.ones((t, 1))], axis=1)\n",
    "                A_test = np.concatenate([test_factors, np.ones((1, 1))], axis=1)\n",
    "            loadings = np.linalg.lstsq(A_fit, self.tgt_series[:t], rcond=None)[0]\n",
    "\n",
    "            \n",
    "            sim_ar_model = AutoReg(self.tgt_series[:t], lags=nlags, old_names=False,\n",
    "                                  trend='n')\n",
    "            sim_ar_model_fit = sim_ar_model.fit()\n",
    "            \n",
    "            gts.append(self.tgt_series[t])\n",
    "            mean_preds.append(np.mean(self.tgt_series[:t]))\n",
    "            ar_forcast = sim_ar_model_fit.forecast()\n",
    "            ar_preds.append(ar_forcast)\n",
    "            preds.append(A_test@loadings)\n",
    "        preds = np.array(preds).squeeze()\n",
    "        ar_preds = np.array(ar_preds).squeeze()\n",
    "        gts = np.array(gts)\n",
    "        mean_preds = np.array(mean_preds)\n",
    "\n",
    "        if plot_resids:\n",
    "            plt.plot(preds - gts, label='factor resids')\n",
    "            plt.plot(ar_preds - gts, label='ar resids')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"r2 vs ar model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - ar_preds)))\n",
    "        print(\"ar r2 vs mean model\", 1 - np.sum(np.square(gts - ar_preds)) / np.sum(np.square(gts - mean_preds)))\n",
    "        print(\"r2 vs mean model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - mean_preds)))\n",
    "        print(\"EV\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts)))\n",
    "        \n",
    "\n",
    "        \n",
    "    def fit_ts_reg(self, plot_resids=False):\n",
    "        self.test_start\n",
    "        gamma_is = SPCA.get_gamma_is(self.data_series[:self.test_start,:], self.tgt_series[:self.test_start])\n",
    "        factors = SPCA.fit_factors(self.data_series*gamma_is, self.n_factors)[self.test_start:]\n",
    "        loadings = np.linalg.lstsq(factors, self.tgt_series[self.test_start:], rcond=None)[0]\n",
    "        preds = factors.dot(loadings)\n",
    "        gts = self.tgt_series[self.test_start:]\n",
    "        if plot_resids:\n",
    "            plt.plot(preds - gts, label='factor resids')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"r2 vs mean model\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - np.mean(gts))))\n",
    "        print(\"EV\", 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts)))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def fit_factors(scaled_data, n_factors):\n",
    "        T,N = scaled_data.shape\n",
    "\n",
    "        fit_pipe = skpipe.Pipeline([('loadings', skd.TruncatedSVD(n_factors, algorithm='arpack'))])\n",
    "        objective = scaled_data.T.dot(scaled_data)\n",
    "        loadings = fit_pipe.fit_transform(objective)\n",
    "        factors = (1/N ) * scaled_data.dot(loadings)\n",
    "        \n",
    "        return factors, loadings\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gamma_is(data, target):\n",
    "        loadings = []\n",
    "        T = target.shape[0]\n",
    "        \n",
    "        for i in range(data.shape[1]):\n",
    "            A = np.stack([data[:,i], np.ones(T)], axis=1)\n",
    "            loading = np.linalg.lstsq(A, target, rcond=None)[0][0]\n",
    "            loadings.append(loading)\n",
    "        return np.array(loadings)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "expired-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GX_SPCA:\n",
    "    \n",
    "    def __init__(self, data_panel, target_panel, test_start, N_factors=6):\n",
    "        \"\"\"\n",
    "        Auguments:\n",
    "        1) data_panel: numpy 2d array of data, normalizations and transforms should have already been applied\n",
    "        2) target_panel: numpy 2d array of target data, normalizations and transforms should have already \n",
    "            been applied\n",
    "        3) n_train: index of start of oos\n",
    "        4) N_factors: number of factors\n",
    "        \"\"\"\n",
    "        T,N = data_panel.shape\n",
    "        self.all_tgt = target_panel\n",
    "        self.train_data = scale(data_panel[:test_start])\n",
    "        self.train_target = target_panel[:test_start]\n",
    "        self.test_data = scale(data_panel[test_start:], ref=data_panel[:test_start])\n",
    "        self.test_target = target_panel[test_start:]\n",
    "        self.n_factors = N_factors\n",
    "        \n",
    "    def fit(self, nlags, true_oos=False, pca=False, raw_data=None,\n",
    "           stack_lags=False, plot_resids=False, quantile=None, print_res=False):\n",
    "        \n",
    "        if quantile is None:\n",
    "            qtile_test_start = self.train_data.shape[0]//2\n",
    "            quantiles = [i*(1/50) for i in range(50)]\n",
    "            qtuile_r2s = [GX_SPCA(self.train_data, self.train_target, \n",
    "                                  qtile_test_start, self.n_factors).fit(nlags=nlags, true_oos=True,\n",
    "                                                                       quantile=q)[0] for q in quantiles]\n",
    "            quantile = quantiles[np.argmax(qtuile_r2s)]\n",
    "            \n",
    "        betas = GX_SPCA.get_portfolio_loadings(self.train_data, self.train_target, self.n_factors,\n",
    "                                              quantile=quantile)\n",
    "        \n",
    "        train_factors = self.train_data @ betas\n",
    "        test_factors = self.test_data @ betas\n",
    "        \n",
    "        preds = []\n",
    "        mean_preds = []\n",
    "        gts = []\n",
    "        for t in range(self.test_data.shape[0]):\n",
    "            if t > 0:\n",
    "                fit_factors = np.concatenate([train_factors, test_factors[:t]], axis=0)\n",
    "                fit_target = np.concatenate([self.train_target, self.test_target[:t]], axis=0)\n",
    "            else:\n",
    "                fit_factors = train_factors\n",
    "                fit_target = self.train_target\n",
    "            \n",
    "            eval_factors = test_factors[t:t+1]\n",
    "            \n",
    "            A_fit = np.concatenate([fit_factors, np.ones((fit_factors.shape[0], 1))], axis=1)\n",
    "            A_test = np.concatenate([eval_factors, np.ones((1, 1))], axis=1)\n",
    "            \n",
    "            loadings = np.linalg.lstsq(A_fit, fit_target, rcond=None)[0]\n",
    "        \n",
    "            gts.append(self.test_target[t])\n",
    "            mean_preds.append(np.mean(self.all_tgt[:t + self.train_target.shape[0]]))\n",
    "        \n",
    "            preds.append(A_test@loadings)\n",
    "\n",
    "        preds = np.array(preds).squeeze()\n",
    "        mean_preds = np.array(mean_preds)\n",
    "        gts = np.array(gts)\n",
    "        r2 =  1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts - mean_preds))\n",
    "        ev = 1 - np.sum(np.square(preds - gts)) / np.sum(np.square(gts))\n",
    "        if print_res:\n",
    "            print(\"r2 vs mean model\", r2)\n",
    "            print(\"EV\", ev)\n",
    "        else:\n",
    "            return r2, ev\n",
    "        \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_portfolio_loadings(train_data, train_tgt, nfactor, quantile):\n",
    "        T, N = train_data.shape\n",
    "        loadings = np.zeros((N, nfactor))\n",
    "        \n",
    "        R_k = np.copy(train_data.T)\n",
    "        G_k = np.copy(train_tgt.T)\n",
    "        rbar_k = np.mean(R_k, axis=1)\n",
    "        \n",
    "        eta_hats = []\n",
    "        gamma_hats = []\n",
    "        v_hats = []\n",
    "        beta_hats = []\n",
    "        \n",
    "        for k in range(nfactor):\n",
    "            vhat, gamma_hat, eta_hat, beta_hat, R_k, rbar_k, G_k = GX_SPCA.get_next_factor_loading(R_k, G_k, rbar_k,\n",
    "                                                                                                  quantile=quantile)\n",
    "            eta_hats.append(eta_hat)\n",
    "            gamma_hats.append(gamma_hat)\n",
    "            v_hats.append(vhat)\n",
    "            beta_hats.append(beta_hat)\n",
    "        \n",
    "        betas = np.concatenate(beta_hats, axis=1)\n",
    "        return betas\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_next_factor_loading(train_data, train_tgt, rbar, quantile):\n",
    "        #train_data: n times T\n",
    "        #train_tgt: 1 times T\n",
    "        N,T = train_data.shape \n",
    "        cors = (1/T)*(train_data @ train_tgt.T)\n",
    "        thresh = np.quantile(cors, quantile) # FIXME\n",
    "        selected_inds = np.argwhere(cors >= thresh).squeeze()\n",
    "#         print(cors.shape, selected_inds.shape)\n",
    "        selected_data = train_data[selected_inds,:] #\n",
    "        \n",
    "        # algo 1\n",
    "        psi, singval, xi = randomized_svd(selected_data, n_components=1,\n",
    "                                           n_iter=10, random_state=None)\n",
    "        vhat = np.sqrt(T) * xi #factors\n",
    "        gamma_hat = (singval/T)*psi.T @ rbar[selected_inds]\n",
    "        eta_hat = (1/T) * train_tgt @ vhat.T\n",
    "        #END algo 1\n",
    "        \n",
    "        beta_hat = (1/T) * train_data @ vhat.T\n",
    "        \n",
    "        train_data_perp = train_data - beta_hat @ vhat\n",
    "        rbar_perp = rbar - beta_hat.squeeze() * gamma_hat\n",
    "        target_perp = train_tgt - eta_hat * vhat.squeeze()\n",
    "        \n",
    "        return vhat, gamma_hat, eta_hat, beta_hat, train_data_perp, rbar_perp, target_perp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-pride",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "internal-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "limiting-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lags(target_series_train, max_lags):\n",
    "    bics = []\n",
    "    for i in range(max_lags):\n",
    "        i += 1\n",
    "        model = AutoReg(target_series_train, lags=i, old_names=False, trend='n')\n",
    "        model_fit = model.fit()\n",
    "        bics.append(model_fit.bic)\n",
    "    return np.argmax(bics) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "proof-journey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VXOCLSx 1\n",
      "SPCA\n",
      "r2 vs ar model -2.984284419953151\n",
      "ar r2 vs mean model 0.77591764844881\n",
      "r2 vs mean model 0.10719217792812852\n",
      "EV 0.8684778894213432\n",
      "PCA\n",
      "r2 vs ar model -3.0016532633523614\n",
      "ar r2 vs mean model 0.77591764844881\n",
      "r2 vs mean model 0.10330012665550903\n",
      "EV 0.8679045400563394\n",
      "giglio xui\n",
      "r2 vs mean model 0.12043065916065077\n",
      "EV 0.8704280885005994\n",
      "\n",
      "UNRATE 1\n",
      "SPCA\n",
      "r2 vs ar model 0.14731217018472953\n",
      "ar r2 vs mean model 0.003061115352460897\n",
      "r2 vs mean model 0.14992234599143361\n",
      "EV 0.14720830611166014\n",
      "PCA\n",
      "r2 vs ar model 0.0910458611534739\n",
      "ar r2 vs mean model 0.003061115352460897\n",
      "r2 vs mean model 0.09382827462257981\n",
      "EV 0.09093514340215536\n",
      "giglio xui\n",
      "r2 vs mean model 0.08937822829189479\n",
      "EV 0.08647088942449666\n",
      "\n",
      "INDPRO 5\n",
      "SPCA\n",
      "r2 vs ar model -0.0391515933196358\n",
      "ar r2 vs mean model 0.11179035309279373\n",
      "r2 vs mean model 0.07701553021450547\n",
      "EV 0.12154856388274904\n",
      "PCA\n",
      "r2 vs ar model -0.027505101440528756\n",
      "ar r2 vs mean model 0.11179035309279373\n",
      "r2 vs mean model 0.08736005665415492\n",
      "EV 0.13139397775951212\n",
      "giglio xui\n",
      "r2 vs mean model 0.08369197394438521\n",
      "EV 0.12790287619748597\n",
      "\n",
      "CPIAUCSL 1\n",
      "SPCA\n",
      "r2 vs ar model 0.05365085462078234\n",
      "ar r2 vs mean model -0.01942888224606132\n",
      "r2 vs mean model 0.035264348511548804\n",
      "EV 0.0333713957759203\n",
      "PCA\n",
      "r2 vs ar model -0.04607583243632307\n",
      "ar r2 vs mean model -0.01942888224606132\n",
      "r2 vs mean model -0.06639991660517874\n",
      "EV -0.0684923495284333\n",
      "giglio xui\n",
      "r2 vs mean model 0.03451626798106766\n",
      "EV 0.03262184740180296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tgt in [\"VXOCLSx\", \"UNRATE\", \"INDPRO\", \"CPIAUCSL\"]:\n",
    "    spca_data = SPCAData(target=tgt, \n",
    "                         drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]\n",
    "#                          drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\"]\n",
    "                        )\n",
    "    data_panel, tgt_data, test_start, raw_data = spca_data.get_data()\n",
    "    opt_lags = get_lags(tgt_data[:test_start], max_lags=5)\n",
    "    print(tgt, opt_lags)\n",
    "    spca = SPCA(data_panel, tgt_data, test_start, N_factor=4)\n",
    "    print(\"SPCA\")\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, fit_factors_epanding_window=False)\n",
    "    print(\"PCA\")\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, pca=True, fit_factors_epanding_window=False)\n",
    "    print(\"giglio xui\")\n",
    "    spca = GX_SPCA(data_panel, tgt_data, test_start, N_factors=4)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, print_res=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "excited-drain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB-yeild-1 4\n",
      "r2 vs ar model -121.13084488570135\n",
      "ar r2 vs mean model 0.9942046744466398\n",
      "r2 vs mean model 0.29221199378042984\n",
      "EV 0.6139277027419132\n",
      "r2 vs ar model -132.9845067168959\n",
      "ar r2 vs mean model 0.9942046744466398\n",
      "r2 vs mean model 0.22351616446921807\n",
      "EV 0.5764566571729361\n",
      "giglio xui\n",
      "r2 vs mean model 0.223516150234484\n",
      "EV 0.5764566494084129\n",
      "\n",
      "FB-yeild-2 5\n",
      "r2 vs ar model -112.0239820703763\n",
      "ar r2 vs mean model 0.9930864643215482\n",
      "r2 vs mean model 0.218604667435765\n",
      "EV 0.6248174227472094\n",
      "r2 vs ar model -117.83199065553454\n",
      "ar r2 vs mean model 0.9930864643215482\n",
      "r2 vs mean model 0.17845079286151955\n",
      "EV 0.6055377655472962\n",
      "giglio xui\n",
      "r2 vs mean model 0.17845087538710724\n",
      "EV 0.6055378051714932\n",
      "\n",
      "FB-yeild-3 5\n",
      "r2 vs ar model -113.7965982147987\n",
      "ar r2 vs mean model 0.9920441066353413\n",
      "r2 vs mean model 0.08669050597749284\n",
      "EV 0.6135411609990893\n",
      "r2 vs ar model -106.35800945954183\n",
      "ar r2 vs mean model 0.9920441066353413\n",
      "r2 vs mean model 0.1458711248978658\n",
      "EV 0.6385829167554999\n",
      "giglio xui\n",
      "r2 vs mean model 0.14587122132186314\n",
      "EV 0.6385829575564621\n",
      "\n",
      "FB-yeild-4 5\n",
      "r2 vs ar model -106.71756666325531\n",
      "ar r2 vs mean model 0.9908695570077629\n",
      "r2 vs mean model 0.016490898318654912\n",
      "EV 0.6337115843506976\n",
      "r2 vs ar model -94.9451965003626\n",
      "ar r2 vs mean model 0.9908695570077629\n",
      "r2 vs mean model 0.12397785297445907\n",
      "EV 0.6737429640873328\n",
      "giglio xui\n",
      "r2 vs mean model 0.12397758229170475\n",
      "EV 0.6737428632769213\n",
      "\n",
      "FB-yeild-5 5\n",
      "r2 vs ar model -107.43563375829119\n",
      "ar r2 vs mean model 0.9903080633410175\n",
      "r2 vs mean model -0.05095129396198783\n",
      "EV 0.6481020375455578\n",
      "r2 vs ar model -91.13208574420382\n",
      "ar r2 vs mean model 0.9903080633410175\n",
      "r2 vs mean model 0.10706166070722767\n",
      "EV 0.7010107090596102\n",
      "giglio xui\n",
      "r2 vs mean model 0.19030700953975743\n",
      "EV 0.7288843781880255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tgt = f'FB-yeild-{i+1}'\n",
    "    spca_data = SPCAData(target=tgt, \n",
    "                         drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VXOCLSx\"]\n",
    "#                          drop_cols=[\"ACOGNO\", \"ANDENOx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\"]\n",
    "                        )\n",
    "    data_panel, tgt_data, test_start, raw_data = spca_data.get_data()\n",
    "    opt_lags = get_lags(tgt_data[:test_start], max_lags=5)\n",
    "    print(tgt, opt_lags)\n",
    "    spca = SPCA(data_panel, tgt_data, test_start, N_factor=4)\n",
    "    spca.fit(opt_lags, true_oos=True, stack_lags=False, fit_factors_epanding_window=False)\n",
    "    spca.fit(opt_lags, true_oos=True, pca=True, fit_factors_epanding_window=False)\n",
    "    print(\"giglio xui\")\n",
    "    spca = GX_SPCA(data_panel, tgt_data, test_start, N_factors=4)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, print_res=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-newspaper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-boxing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkt\n",
      "r2 vs ar model 0.972973859808027\n",
      "ar r2 vs mean model -0.008956470002243533\n",
      "r2 vs mean model 0.9727318009941212\n",
      "EV 0.9730444366094004\n",
      "\n",
      "SMB\n",
      "r2 vs ar model 0.5125360009810158\n",
      "ar r2 vs mean model -0.03806164735615991\n",
      "r2 vs mean model 0.4939823181515317\n",
      "EV 0.4941297074762856\n",
      "\n",
      "HML\n",
      "r2 vs ar model 0.6278523209962694\n",
      "ar r2 vs mean model 0.021473849942505452\n",
      "r2 vs mean model 0.6358437644116471\n",
      "EV 0.6329367461257922\n",
      "\n",
      "CMA\n"
     ]
    }
   ],
   "source": [
    "for tgt in 'mkt', 'SMB', 'HML', 'CMA', 'RMW' :\n",
    "    print(tgt)\n",
    "    chen_z_data_panel, tgt_factor, weak_test_start = ChenZData(tgt_factor=tgt).data\n",
    "    spca = SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factor=5)\n",
    "    spca.fit(1, true_oos=False, stack_lags=False, fit_factors_epanding_window=False)\n",
    "    spca = GX_SPCA(chen_z_data_panel, tgt_factor, weak_test_start, N_factors=5)\n",
    "    spca.fit(1, true_oos=True, stack_lags=False, quantile=q/50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-edward",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
